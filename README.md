# ğŸš¦ Smart Traffic Light Control with Machine Learning

This project implements an **AI-powered traffic light controller** that adapts in real time to traffic conditions.  
The goal is to reduce **average waiting time**, **queue lengths**, and **vehicle stops** compared to traditional fixed-time and rule-based controllers.  

---

## âœ¨ Features
- ğŸŸ¢ **Traffic simulation** using [CityFlow](https://github.com/cityflow-project/CityFlow) or [SUMO](https://www.eclipse.org/sumo/).  
- ğŸ¤– **Reinforcement Learning agents** (DQN, PPO) for adaptive signal control.  
- âš–ï¸ **Baseline controllers**:  
  - Fixed-time (static schedule)  
  - Actuated (queue-threshold rules)  
- ğŸ“Š **Performance metrics**: average delay, queue length, throughput, stop count, emissions proxy.  
- ğŸ”’ **Safety constraints**: min/max green, amber, pedestrian phases.  
- ğŸ“¹ (Optional) **Camera-based detection pipeline** with lightweight models (YOLO/OpenCV).  

---

## ğŸ› ï¸ Tech Stack
- **Python 3.9+**  
- [PyTorch](https://pytorch.org/) â€“ RL agent training  
- [Gymnasium](https://gymnasium.farama.org/) â€“ environment API  
- [CityFlow](https://github.com/cityflow-project/CityFlow) or [SUMO](https://www.eclipse.org/sumo/) â€“ traffic simulation  
- [Matplotlib](https://matplotlib.org/) â€“ visualization  
- [Weights & Biases](https://wandb.ai/) (optional) â€“ experiment tracking  

---

## ğŸ“‚ Project Structure

```plaintext
traffic-light-ml/
â”œâ”€â”€ README.md               # Project documentation
â”œâ”€â”€ requirements.txt        # Dependencies list
â”œâ”€â”€ configs/                # CityFlow/SUMO configs
â”‚   â””â”€â”€ single_intersection.json
â”œâ”€â”€ envs/                   
â”‚   â””â”€â”€ traffic_env.py      # Custom Gym environment wrapper
â”œâ”€â”€ controllers/            
â”‚   â”œâ”€â”€ fixed_time.py       # Fixed-time baseline controller
â”‚   â”œâ”€â”€ actuated.py         # Rule-based (actuated) controller
â”œâ”€â”€ agents/                 
â”‚   â”œâ”€â”€ dqn_agent.py        # Deep Q-Network (DQN) agent
â”‚   â””â”€â”€ ppo_agent.py        # Proximal Policy Optimization (PPO) agent
â”œâ”€â”€ train.py                # Training script
â”œâ”€â”€ evaluate.py             # Evaluation script
â”œâ”€â”€ utils/                  
â”‚   â”œâ”€â”€ logger.py           # Logging & plotting utilities
â”‚   â””â”€â”€ replay_buffer.py    # Replay buffer for DQN
â””â”€â”€ data/                   
    â””â”€â”€ results/            # Saved logs and experiment outputs
```

1) Define the smallest useful goal
	â€¢	Scope: start with one 4-way intersection; scale to corridors later.
	â€¢	Objective: minimize average vehicle delay and queue length; secondary: reduce stops and emissions.

2) Choose a simulator (so you can iterate safely)

Pick one:
	â€¢	SUMO (mature, open-source; Python API via TraCI)
	â€¢	CityFlow (fast, RL-friendly; simple JSON configs)
Either lets you spawn cars with routes, read queues/wait time, and switch signal phases from Python.

3) Baselines (so ML has something to beat)

Implement two non-ML controllers:
	1.	Fixed-time (e.g., NS/EW green 30s, then 10s yellow, etc.)
	2.	Actuated (simple rules using queue thresholds or detector hits)

Measure:
	â€¢	Avg delay (s/veh)
	â€¢	Avg queue length
	â€¢	Throughput (veh/hour)
	â€¢	% time green wasted (green with no cars)

4) Formulate the ML problem (Reinforcement Learning)

Environment
	â€¢	State (each step, e.g., every 2â€“5s):
	â€¢	Queue lengths per approach/lane
	â€¢	Waiting time per lane
	â€¢	Current phase + time in phase
	â€¢	(optional) approaching vehicles within 100m
	â€¢	Action:
	â€¢	Either: choose next phase (from a valid set)
	â€¢	Or: extend/terminate current green (binary), with min/max green constraints
	â€¢	Reward (maximize):
	â€¢	r = - (sum of delays + queue penalties + stop penalties)
	â€¢	Add small penalty for phase change to discourage flicker
	â€¢	Constraints (hard rules, always enforced):
	â€¢	Minimum green, yellow, and all-red durations
	â€¢	No conflicting greens (safety first)
